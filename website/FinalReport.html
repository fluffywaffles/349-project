<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">ol{margin:0;padding:0}table td,table th{padding:0}.c11{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c6{font-size:8pt;font-style:italic;text-decoration:underline}.c1{orphans:2;widows:2;text-align:center}.c3{orphans:2;widows:2;height:11pt}.c4{orphans:2;widows:2}.c2{color:#1155cc;text-decoration:underline}.c14{color:inherit;text-decoration:inherit}.c9{text-decoration:underline}.c15{font-size:12pt}.c10{height:11pt}.c13{font-size:9pt}.c5{background-color:#ffff00}.c16{font-size:16pt}.c12{font-size:10pt}.c8{font-style:italic}.c0{font-weight:bold}.c7{text-indent:36pt}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c11"><p class="c1"><span class="c0 c15">Generating Game Reviews</span></p><p class="c3"><span></span></p><p class="c1"><span>Michael Horst, Jordan Timmerman, Agam Gupta, Upasna Madhok</span></p><p class="c3"><span></span></p><p class="c1"><span>Contact: </span><span class="c2"><a class="c14" href="mailto:MichaelHorst2016@u.northwestern.edu">MichaelHorst2016@u.northwestern.edu</a></span></p><p class="c3"><span></span></p><p class="c1"><span>EECS 349: Machine Learning</span></p><p class="c1"><span>Northwestern University, Spring 2016</span></p><p class="c3"><span></span></p><p class="c1"><span class="c0">Motivation</span></p><p class="c4 c7"><span>Steam (an online gaming platform that provides users with installation and automatic updating of games in addition to many other features) has a database of games having tens of thousands of reviews, not all of which are high quality/useful and a lot of which are similar to one another. We think generating reviews that can accurately emulate the best reviews on Steam will give prospective buyers more information, especially if the model can generate high-quality reviews of a game based upon a data set consisting of mixed quality reviews.</span><span class="c0">&nbsp;</span><span>This will save potential customers time in evaluating whether a game is worth purchasing.</span><span class="c8"><br></span></p><p class="c1"><span class="c0">Solution</span></p><p class="c4 c7"><span>We focused on two different machine learning solutions to generate helpful reviews: character-level Recurrent Neural Networks (char-RNN) and Hidden Markov Models (HMM). char-RNNs train on a chunk of text and learn to generate new text character by character, whereas HMMs calculate transition probabilities between n-grams (sequences of n words) for training, and then can be sampled to generate text. To train char-RNN, we used text files containing review content ranging from 300 kB to 15 mB, epochs ranging from 10-50 (more for smaller data sets) and hidden units ranging from 48-128 (more for larger data sets). We then generated samples with temperatures ranging from 0.3 to 0.6. For HMMs, we wrote generic model generation and sampling scripts and trained multiple models. First, we tried taking the top 50 most helpful reviews for a specific game and creating bigrams and trigrams. Then, we tried taking a sample of 15000 out of the set of top 50 most helpful reviews from every game in the data set of 8145 games, and created models with bigrams, trigrams, all the way up to 10-grams, and tested the output. 10-gram models generated almost verbatim copies of very long reviews from the data set. 3-5-gram models generated coherent, but sometimes confused, reviews that were fairly original but which might talk about one game one paragraph, and another game in the next - and mention each by name. Results were better when training data was limited to a set of reviews for a particular game, although the results were often passable even with the large training set.</span></p><p class="c3 c7"><span></span></p><p class="c1"><span class="c0">Testing/Training</span></p><p class="c4 c7"><span>The dataset of the games was extracted from the steam API (scraping from </span><span class="c2"><a class="c14" href="https://www.google.com/url?q=http://steamcommunity.com/app/%257Bgameid%257D/homecontent&amp;sa=D&amp;ust=1465448060017000&amp;usg=AFQjCNGURsW0YN15TxDVBChhyQiY9Lsqmg">http://steamcommunity.com/app/{gameid}/homecontent</a></span><span>&nbsp;URLs). This HTML content was then scraped and processed using a programmatic headless web browser (CasperJS) to extract information such as &lsquo;</span><span class="c8">reviewerUsername&#39;, &#39;reviewerDisplayName&#39;, &#39;reviewerProductsCount&rsquo;, &#39;reviewText&#39;, &#39;recommended&#39;, &#39;hoursPlayed&#39;, &#39;commentsCount&#39; - &nbsp;</span><span>for every review of every game title. The reviews were saved as csvs, each csv containing all reviews for a particular game. To train the Hidden Markov Model we used the top 50 most helpful reviews from each of 8145 games; whereas for training the RNN, we used the top 1000 most helpful reviews for a single game. Since we are using text generation algorithms, we don&rsquo;t have a test set to compare our models against. </span></p><p class="c3"><span></span></p><p class="c1"><span class="c0">Key Results</span></p><p class="c4 c7"><span>We considered success to be when the generated review makes sense and has a positive or negative sentiment to it. We compare this tone with the general sentiment of the review (provided in the training dataset). If the sentiments match then we achieve success. This was done for multiple games.</span></p><p class="c3"><span class="c8"></span></p><p class="c4"><span class="c9 c8 c0">HMM:</span><span class="c8 c0">&nbsp;</span><span>The results from the Hidden Markov Models turned out very well. In particular, they were clear, grammatically plausible and (usually) made sense contextually as reviews. (See below for an example of a review generated by HMM.) Given the quality of these generated reviews we have concluded that HMM works as a better model for generating game reviews from a corpus of existing reviews.</span></p><p class="c3"><span class="c9 c8 c0"></span></p><p class="c4"><span class="c9 c8 c0">Char-RNN Model:</span><span class="c8 c0">&nbsp;</span><span>We tried a couple of games on the charRNN model and compared the resulting review&rsquo;s sentiment to the original one. We could not get a good accuracy for the games that we tested on, as the tone of the review could not be identified (in the interest of time, we only tested our model on a few reviews). The perplexity of the model for these games was roughly 8%. When testing on a much larger test set of the top 3 reviews of every game (roughly 15 MB of text) the resultant reviews were higher quality, but had no relation to any particular game.</span></p><p class="c3"><span></span></p><p class="c4"><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 186.67px;"><img alt="" src="images/image01.png" style="width: 624.00px; height: 186.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c9 c13 c8">Figure 1: One of our best HMM reviews with a 5-gram HMM, generated from 15,000 reviews</span></p><p class="c1 c10"><span class="c12"></span></p><p class="c4"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 644.00px; height: 195.50px;"><img alt="Screen Shot 2016-06-08 at 6.28.10 PM.png" src="images/image03.png" style="width: 644.00px; height: 195.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c9 c8 c13">Figure 2: One of our best char-RNN reviews for a single game, the top reviews multiplied for extra content</span></p><p class="c3"><span class="c5"></span></p><p class="c3"><span></span></p><p class="c3"><span></span></p><p class="c4"><span class="c5">***************************** &nbsp; &nbsp; &nbsp;Abstract ends (Remove everything above this line, including this line from final report which needs to be as a pdf) &nbsp; **********************</span></p><p class="c1"><span class="c0">Generating Game Reviews</span></p><p class="c3"><span></span></p><p class="c1"><span>Michael Horst, Jordan Timmerman, Agam Gupta, Upasna Madhok</span></p><p class="c3"><span></span></p><p class="c1"><span>Contact: </span><span class="c2"><a class="c14" href="mailto:MichaelHorst2016@u.northwestern.edu">MichaelHorst2016@u.northwestern.edu</a></span></p><p class="c3"><span></span></p><p class="c1"><span>EECS 349: Machine Learning</span></p><p class="c1"><span>Northwestern University, Spring 2016</span></p><p class="c3"><span></span></p><p class="c1"><span class="c0">Motivation</span></p><p class="c4 c7"><span>Steam (an online gaming platform that provides users with installation and automatic updating of games in addition to many other features) has a database of games that have over tens of thousands of reviews, not all of which are high quality/useful and a lot of which are similar to one another. We think generating reviews that can accurately emulate the best reviews on Steam will give prospective buyers more information, especially if the model can generate high-quality reviews of a game based upon a data set of lower quality reviews.</span><span class="c0">&nbsp;</span><span>This will save potential customers time in evaluating whether a game is worth purchasing.</span><span class="c8"><br></span></p><p class="c1"><span class="c0">Solution</span></p><p class="c4 c7"><span>We focused on two different machine learning solutions to generate helpful reviews: character-level Recurrent Neural Networks (char-RNN) and Hidden Markov Models (HMM). char-RNNs train on a chunk of text and learn to generate new text character by character, whereas HMMs calculate transition probabilities between n-grams (sequences of n words) for training, and then can be sampled to generate text. To train char-RNN, we used text files containing review content ranging from 300 kB to 15 mB, epochs ranging from 10-50 (more for smaller data sets) and hidden units ranging from 48-128 (more for larger data sets). We then generated samples with temperatures ranging from 0.3 to 0.6. For HMMs, we wrote generic model generation and sampling scripts and trained multiple models. First, we tried taking the top 50 most helpful reviews for a specific game and creating bigrams and trigrams. Then, we tried taking a sample of 15000 out of the set of top 50 most helpful reviews from every game in the data set of 8145 games, and created models with bigrams, trigrams, all the way up to 10-grams, and tested the output. 10-gram models generated almost verbatim copies of very long reviews from the data set. 3-5-gram models generated coherent, but sometimes confused, reviews that were fairly original but which might talk about one game one paragraph, and another game in the next - and mention each by name. Results were better when training data was limited to a set of reviews for a particular game, although the results were often passable even with the large training set.</span></p><p class="c3"><span></span></p><p class="c1"><span class="c0">Training</span></p><p class="c4"><span>Our data was extracted from the Steam API. We first obtained a list of all games with their appID (the game ID), review score, review sentiment and reviewcount. 8145 games were scraped. We then iterated over all these IDs, calling an endpoint for &lsquo;homeContent&rsquo; from which could be accessed HTML reviews, which were then parsed and processed to generate a dataset for every review of every game containing </span><span class="c8">reviewerUsername&#39;, &#39;reviewerDisplayName&#39;, &#39;reviewerProductsCount&#39;, &#39;reviewText&#39;, &#39;recommended&#39;, &#39;hoursPlayed&#39;, &#39;commentsCount&#39;.</span><span>&nbsp;For each game</span><span>, the reviewText columns of the review CSVs contains the actual text content of reviews by different users (with newlines escaped; our RNN and HMM then unescape newlines during generation)</span><span>.</span></p><p class="c3"><span></span></p><p class="c4"><span>For Char-RNN, data was preprocessed u</span><span>sing </span><span class="c8">pandas</span><span>&nbsp;and other useful libraries.</span><span>&nbsp;W</span><span>e would store selected reviews</span><span>, weigh </span><span>them based on different approaches, and then dump them to </span><span>a text file, which is then used to train the CharRNN model. This was done for every game in the database.</span></p><p class="c3"><span></span></p><p class="c4"><span>For the HMM we use a slightly different approach. Bash scripts using GNU Parallel, awk, and other unix utilities were written for slicing and dicing the review csvs. These scripts were then used to take the top 10, 25, and 50 most helpful reviews from every game and dump those into their own csv files. The HMM was trained using one of those data sets, and then the model was dumped to a python pickle file. A separate HMM sampling script was then run to generate reviews from the model. </span></p><p class="c3"><span></span></p><p class="c1"><span class="c0">Methods and Models</span></p><p class="c4"><span>We used these machine learning techniques because char-RNN and HMM are unsupervised learning algorithms that generate statistical models which can then be sampled to create psuedo-original text. Unsupervised learning was necessary because the variables going into the creation of a review were unknown, and statistical models provide a convenient sampling source for text generation.</span></p><p class="c3"><span></span></p><p class="c4"><span class="c0">Char RNN Model</span></p><p class="c3"><span></span></p><p class="c4"><span>We tried different ways of creating the training text file since that is used for training the model. We used a couple of different approaches with varying values of temperature, number of epochs and number of hidden units so as to generate the least possible perplexity. </span></p><p class="c3"><span></span></p><p class="c4"><span>The various approaches used were as follows: </span></p><p class="c4"><span>&nbsp;</span></p><p class="c4"><span class="c9">Approach 1: </span><span class="c9 c8">Use the top 1000 reviews of a particular game as the training set</span></p><p class="c4"><span>In this scenario, we took the top 1000 reviews of a particular game (such as CounterStrike or Dota - each having over a million reviews) and stored them into a text file. This was used to train the RNN model.</span></p><p class="c3"><span></span></p><p class="c4"><span class="c9 c8">Approach 2: Weighting the reviews</span></p><p class="c4"><span>In this approach, we use a weighting function to give useful reviews a higher priority in the training model. Since some of the reviews are gibberish or not useful this approach should help in generating more meaningful text. The following weight function was used: </span></p><p class="c3"><span></span></p><p class="c4"><span class="c8">review_textFile = </span><img src="images/image00.png"></p><p class="c3"><span></span></p><p class="c4"><span class="c9 c8">Approach 3: Combining the top 3 reviews from all the games</span></p><p class="c4"><span>To have a considerable amount of data to train on, we took the top 3 reviews from each game title and stored them all in one text file. Even this amounted to about 12 MB of data, a formidable amount on which to train. We had initially considered the top 50, but that amounted to 160 MB, not feasible for the computing power we have available. To demarcate a particular game, we added the name of the game prior to every review corresponding to that game in the text file. This was done so that we have adequate information to train on and when we use the model to generate a sample, we can potentially ask it to generate reviews for a particular game based on keyword search.</span></p><p class="c3"><span></span></p><p class="c3"><span class="c5"></span></p><p class="c4"><span class="c0">H</span><span class="c0">idden Markov Model</span></p><p class="c3"><span class="c5 c0"></span></p><p class="c4"><span>We created word-level Hidden Markov Models, as opposed to character level (like Char-RNN) both for the purposes of contrasting the two approaches, and because the contextual nature of n-grams lends itself fairly well to sentence generation from probabilistic sequences of words. We experimented with varying values of &ldquo;n&rdquo; for our n-grams, from 2-10, on sets of up to 15,000 reviews taken from the set of 279,520 top 50 most helpful reviews sampled from 8,145 games. We found that unigram-generated reviews were largely unintelligible, and that reviews generated with 5-gram and above became borderline plagiaristic. Bigram models worked well, as did trigram models, but 4-gram performed best overall. Larger values of n were less likely to produce short reviews (which makes sense), and so using a variety of values of n helps to generate the most realistic set of reviews in total. We were able to generate game-specific reviews by taking data only from a specific game (given that enough data was available), and these tended to be pretty good - see the following reviews generate for Call of Duty: World at War (there are 4 examples; all 4-gram):</span></p><p class="c3"><span></span></p><p class="c4"><span style="overflow: hidden; display: inline-block; margin: 0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 124.00px;"><img alt="" src="images/image04.png" style="width: 624.00px; height: 124.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c6">Figure 1: examples of reviews generated for a specific game</span></p><p class="c3"><span class="c9 c0"></span></p><p class="c4"><span>We also generated reviews on a cross-sectional data set of 15,000 of the 279,520 top 50 most helpful reviews from all 8,145 games, and here are two examples (both generated from a 4-gram model):</span></p><p class="c3"><span></span></p><p class="c4"><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 186.67px;"><img alt="" src="images/image01.png" style="width: 624.00px; height: 186.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c3"><span class="c9 c8 c0"></span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: -0.00px -0.00px; border: 1.33px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 157.33px;"><img alt="" src="images/image05.png" style="width: 624.00px; height: 157.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c6">Figures 2 &amp; 3: some of the better reviews generated from a 4-gram model</span></p><p class="c1 c10"><span class="c9 c8 c0"></span></p><p class="c4"><span>Here&rsquo;s an example 4-gram model output that shows cross-game confusion (mentioning both SPAZ and Batman: Arkham Asylum) and bullet-list generation (&lsquo;+&rsquo;s are &ldquo;pro&rdquo; and &lsquo;-&rsquo;s are &ldquo;con&rdquo;; this is a known convention in the Steam community):</span></p><p class="c4"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 385.33px;"><img alt="hmm-list-ex.png" src="images/image02.png" style="width: 624.00px; height: 385.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c6">Figure 4: 4-gram model generated text with bullet list and cross-game confusion</span></p><p class="c1 c10"><span class="c8 c9"></span></p><p class="c1"><span class="c0">Results &amp; Analysis </span></p><p class="c4"><span>After implementing the HMM and trying different approaches for char RNN we realized that</span><span>&nbsp;</span><span>HMM performs much better at generating sensible and useful results than RNN. </span></p><p class="c3"><span></span></p><p class="c4"><span>The best result we got for char RNN on an individual game was through approach 2, ie., by weighing the inputs, and that led to a perplexity of </span><span class="c8">7.959</span><span>. We learnt that decreasing the value of the hidden units gives lower perplexity as the model does not overfit. Additionally, in char RNN the perplexity goes down exponentially, decreasing extremely fast for the first few epochs and then mostly being stable until it rises again due to overfitting. Thus through multiple tests, a suitable value of 48 hidden units, and 50 epochs was chosen to get the best possible results.</span></p><p class="c3"><span></span></p><p class="c4"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 643.50px; height: 200.00px;"><img alt="Screen Shot 2016-06-08 at 6.28.10 PM.png" src="images/image03.png" style="width: 643.50px; height: 200.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c6">Figure 5: The review summary generated by Char RNN for a temperature of 0.5, hidden units = 48, number of layers = 2</span></p><p class="c3"><span class="c9 c8"></span></p><p class="c4"><span>As seen through figure 3, the generated summarized review doesn&rsquo;t make much sense, and so we could not extract it&rsquo;s sentiment to compare with the average review sentiment from the initial test data. </span></p><p class="c3"><span></span></p><p class="c4"><span>The results from the HMM, trained either on specific game reviews or cross-sections of the most helpful reviews across all games, were much more productive. The summarized text that we got mirrored the general language structure of a game&rsquo;s review fairly closely - sometimes even mimicking bullet-lists of pros and cons. Since large values of n for our n-grams tended to be fairly unoriginal and plagiaristic, and since small values of 1-3 tended to produce short reviews that were sometimes unintelligible (correct grammar, but no meaning), 4-grams proved to be the most effective.</span></p><p class="c3"><span></span></p><p class="c1"><span class="c8">B</span><span class="c8">rief suggestions for future work.</span></p><p class="c4 c7"><span>Our most successful Hidden Markov Models used an aggregate of the top 50 reviews from 8,145 games, and while these tended to look like fairly &ldquo;real&rdquo; reviews they are hard to associate with any particular game. We too the top 50 most helpful reviews because review quality tends to drop off steeply after about that value. Models on specific games tended to perform worse, largely because many games did not have enough high-quality reviews for the model to be effective. (Only 3,332 of the 8,145 games had more than 100 reviews total). Future work could improve upon the more general model by seeding it with the name of a game; providing transition models for common review components such as ratings (eg &ldquo;11/10&rdquo;), pro/con lists, and section headers; or adjusting the weights of the model to favor less common review elements more and discourage repetitive content.</span></p><p class="c3"><span></span></p><p class="c1"><span class="c8">Who worked on what</span></p><p class="c4"><span>Agam, Upasna and Jordan worked on scraping review data from the API. Michael and Agam compiled the text documents of varying review sets and trained the char-RNNs. Jordan created a flexible script to create various review sets in .csv format for HMM training, and programmed the HMM algorithm itself. Everybody worked on the report.</span></p><p class="c3"><span></span></p><p class="c4"><span>Link to GitHub: </span><span class="c2"><a class="c14" href="https://www.google.com/url?q=https://github.com/skorlir/349-project&amp;sa=D&amp;ust=1465448060045000&amp;usg=AFQjCNFcpm_mFoUEfU2Ze3be0WExaSHfsA">https://github.com/skorlir/349-project</a></span></p></body></html>